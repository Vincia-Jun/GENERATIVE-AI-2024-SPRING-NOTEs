# 07-大型语言模型修炼史（第二阶段：名师指点，发挥潜力）

解释了为什么一定要指定角色？确保文字接龙符合预期

如果只用督导室会怎么样？即使上百万笔，还是不够的。
Instruction Fine-tuning



Adapter，作业里面使用 Lora


「舉一反三」的能力可以有多誇張


通才性能：

Instruct → 真实使用者，效果更好

Instruction Fine-tuning 是畫龍點睛

所以我自己也能做 Instruction Fine-tuning 嗎？不行
语料准备
初始参数作为自己的 LLaMA
整个世界被解放了：Alpaca，Vicuna

Vicuna：从网站


LLaMA 子子孙孙开始开枝散叶，飞去平常百姓家

  

## Ⅴ，拓展资料
| 说明🔎   | 链接🔗 | 重要⭐ | TODO✅ |
|--------|----------|--------|--------|
| Adapters 使用文档 | [网页地址](https://adapterhub.ml/) | ⭐⭐⭐⭐ |
| 语言模型在多语言语料的举一反三能力 | [论文地址](https://arxiv.org/abs/1909.09587) | ⭐ |
| Bert 如何通过 Fine-tuning 打造一堆专才 | [视频地址](https://www.youtube.com/watch?v=gh0hewYkjgo) | ⭐⭐ |
| 通往通才的研究：语言模型如何复习学习过的知识？ | [论文地址](https://arxiv.org/abs/1909.03329v2) | ⭐ |
| 通往通才的研究：FLAN | [论文地址](https://arxiv.org/abs/2109.01652) | ⭐ |
| 通往通才的研究：T0 | [论文地址](https://arxiv.org/abs/2110.08207) | ⭐ |
| 通往通才的研究：在更大量的任务(1.8K)中进行 Fine-tuning | [论文地址](https://arxiv.org/abs/2110.08207) | ⭐ |
| Instruction Fine-tuning 是画龙点睛：InstructGPT | [论文地址](https://arxiv.org/abs/2203.02155) | ⭐ |
| Instruction Fine-tuning 是画龙点睛：Llama-2 | [论文地址](https://arxiv.org/abs/2307.09288) | ⭐ |
| Instruction Fine-tuning 是画龙点睛：LIMA | [论文地址](https://arxiv.org/abs/2305.11206) | ⭐ |
| 逆向工程 | [论文地址]() | ⭐ |aa
