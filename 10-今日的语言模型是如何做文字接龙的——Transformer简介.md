# 10-今日的语言模型是如何做文字接龙的——Transformer简介



## 

今天深入语言模型的内部，是为了了解语言模型心里想什么

模型的演进历史！！！

embedding，由语义关系，居然是查表

Attention，其实不是首次提出，而是发现不需要使用 RNN 的技术。

Causual Attention

讲解了 Transformer 的核心概念

 一些未来的研究方向
 





## 📚 拓展资料
| 说明🔎   | 链接🔗 | 重要⭐ | TODO✅ |
|--------|----------|--------|--------|
| 语言模型的演进史 | [视频地址](https://www.youtube.com/watch?v=dymfkWtVUdo)| ⭐ |
| Transformer-视频详解（上） | [视频地址](https://www.youtube.com/watch?v=n9TlOhRjYoc)| ⭐ |
| Transformer-视频详解（下） | [视频地址](https://www.youtube.com/watch?v=N6aRv06iv2g)| ⭐ |
| Attention Is All You Need | [论文地址](https://arxiv.org/abs/1706.03762)| ⭐ | 
