# 11-大型语言模型在「想」什么呢——浅谈大型语言模型的可解释性


## 🎬 引言：从《葬送的芙莉莲》谈起
- 大型语言模型到底在“想”什么？
- 人类可以观察、解释他人的行为，但语言模型的决策过程却是“黑盒子”。

---

## 🔍 可解释性的三个维度
- **Transparency（透明性）**：模型的权重/数据/训练过程是否开源？
  
- **Interpretable（可解释）**：模型本身结构简单易懂（如决策树）。

- **Explainable（可被解释）**：不一定结构简单，但可以用外部方法解释其行为，本课程关注重点。

---

## 🎯 找出影响输出的关键输入

- **黑盒分析-控制变量法：** 改变输入观察对输出影响，即对输入的某些 Token 控制变量。

- **白盒分析-分析注意力：** 可视化 Transformer 各层注意力权重对结果的影响。

- **综合分析-In-context Learning 分析：** 提供多个范例，结合中间过程的注意力权重，判断范例对输出结果的影响。

- **思路启发：** 如果我们能够清楚关键的注意力分配，那么就可以优化计算资源，减少注意力的计算开销。

---


## 📚 找出影响输出的关键训练资料

- 《[Studying Large Language Model Generalization with Influence Functions](https://arxiv.org/abs/2308.03296)》
  
- 分析了对结果影响最大的训练资料。

- 发现了大规模语言模型具有跨语言学习的能力。

---

## 🧬 分析 Embedding 中的信息

- **Probing 技术：** 在不同层提取 embedding，训练分类器判断其是否包含词性等语言知识。
  
- 《[What Does BERT Learn about the Structure of Language?](https://aclanthology.org/P19-1356/)》
  - 这篇文章在 BERT 上进行分析，试图判断不同的 Transformer 层展现的对文本的层次性理解：句子表面特征（Surface）、句法结构（Syntax）、语义信息（Semantic），并发现层级越深的地方对抽象语义的理解能力越强。
  - 语言模型胚胎学，是新颖的研究方向，研究 BERT 在训练过程中究竟学习到了什么，可以参考下面的论文链接详细了解。
 
- **Embedding 可视化分析：** 将高维嵌入投影到某个二维平面，观察相似度与聚类。
  - 《[A Structural Probe for Finding Syntax in Word Representations](https://aclanthology.org/N19-1419/)》 → 投影到语法树平面。
  - 《[Language Models Represent Space and Time](https://arxiv.org/abs/2310.02207)》 → 投影到世界地图平面。
    
- **Embedding 测谎仪：** 将中间的 Embedding 测谎模型判断真假。

---


## 🗣 语言模型的自我解释能力

- 《[Language models can explain neurons in language models](https://openai.com/index/language-models-can-explain-neurons-in-language-models/)》
  - 利用 GPT-4 解读 GPT-2 的行为
 
- 《[Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations](https://arxiv.org/abs/2310.11207)》
  - 直接要求模型自己给出输入 Token 的影响分数

---


### 🧾 课堂小结

这节内容主要探讨了如何对语言模型黑盒进行分析解释，对以后科研工作中的性能分析有较大的启发意义。

---




## 📚 拓展资料
| 说明🔎   | 链接🔗 | 重要⭐ | TODO✅ |
|--------|----------|--------|--------|
| 机器学习的可解释性（上）  | [视频地址](https://www.youtube.com/watch?v=WQY85vaQfTI)| ⭐⭐⭐⭐ |
| 机器学习的可解释性（下）  | [视频地址](https://www.youtube.com/watch?v=0ayIPqbdHYQ)| ⭐⭐⭐⭐ |
| In-Context Learning 解释性分析（1）  | [论文地址](https://arxiv.org/abs/2305.14160)| ⭐ |
| In-Context Learning 解释性分析（2）  | [论文地址](https://arxiv.org/abs/2305.14160)| ⭐ |
| BERT 学习到的语义层次  | [论文地址](https://aclanthology.org/P19-1356/)| ⭐ |
| 语言模型胚胎学（1）  | [论文地址](https://arxiv.org/abs/2010.02480)| ⭐ |
| 语言模型胚胎学（2）  | [论文地址](https://arxiv.org/abs/2104.07885)| ⭐ |
| Embedding → 2D：发现语法树 | [论文地址](https://aclanthology.org/N19-1419/)| ⭐⭐⭐ |
| Embedding → 2D：世界地图| [论文地址](https://arxiv.org/abs/2310.02207)| ⭐⭐⭐ |
| LLM 的测谎分析 | [论文地址](https://arxiv.org/abs/2304.13734)| ⭐ |
| LLM 内部可能在做什么事情 | [论文地址](https://arxiv.org/abs/2401.01286)| ⭐⭐⭐ |
| 用 LM 来解释 LM | [视频地址](https://www.youtube.com/watch?v=GBXm30qRAqg)| ⭐ |
| 用 LM 来解释 LM | [视频地址](https://www.youtube.com/watch?v=OOvhBIIHITE)| ⭐ |
| 语言模型的自我解释 | [论文地址](https://arxiv.org/pdf/2310.11207)| ⭐ |
| 语言模型的输出置信度分析 | [论文地址](https://arxiv.org/pdf/2310.11207)| ⭐ |
| 语言模型的输出置信度分析 | [论文地址](https://arxiv.org/abs/2306.13063)| ⭐ |

