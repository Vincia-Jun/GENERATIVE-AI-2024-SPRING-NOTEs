# 🔒 13 大型语言模型的安全性议题（上）· 课堂笔记


## 🧭 一、课程大纲

1. 大型语言模型还是会讲错话怎么办？（幻觉、虚假信息）
2. 大型语言模型是否自带偏见？（性别、种族、政治等）
3. 如何判断一段话是不是由 AI 生成？
4. 大型语言模型也会被用于诈骗或误用！

> 注：本课程重点是指出问题，具体解决方案详见引用文献。

---

## 🧙 二、语言模型的错误与幻觉

- 模型幻觉（Hallucination）：模型输出看似合理，实则虚构、错误

- 修补机制：模型原始输出和人类可见中间加入安全层，包括事实查核、有害内容识别等功能模块。

- 《[FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation](https://arxiv.org/abs/2305.14251)》

- 《[FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios](https://arxiv.org/abs/2307.13528)》，将陈述的关键观点抽取出来利用搜索引擎给出置信度评分。

- 《[Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations](https://arxiv.org/abs/2402.05629)》，研究整体性幻觉：所有的链接和资讯都是正确的，但全部内容混杂在一起整体来看是错的。

---


## ⚖️ 三、偏见与歧视问题

### 📐 3.1 偏见检测方法
- **偏见输入设计：** 通过修改性别、种族等敏感词汇来探测差。
- 《[Holistic Evaluation of Language Models](https://arxiv.org/abs/2211.09110)》
![image](https://github.com/Vincia-Jun/GENERATIVE-AI-2024-SPRING-NOTEs/blob/main/Figs/13-%E5%AE%89%E5%85%A8%E6%80%A7%E8%AE%AE%E9%A2%98-00.png)

- **训练红队模型：** 训练红队诱导模型，产生引导偏见的输入。
  - 《[Learning from Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models](https://arxiv.org/abs/2310.11079)》
  - 《[Red Teaming Language Models with Language Models](https://arxiv.org/abs/2202.03286)》
![image](https://github.com/Vincia-Jun/GENERATIVE-AI-2024-SPRING-NOTEs/blob/main/Figs/13-%E5%AE%89%E5%85%A8%E6%80%A7%E8%AE%AE%E9%A2%98-01.png)


### 🧑‍💼 3.2 用大型语言模型审查履历
- Bloomberg 报告：GPT 模拟 HR 招聘时存在种族/性别偏好。
- https://www.bloomberg.com/graphics/2024-openai-gpt-hiring-racial-discrimination/

### 👩‍🏫 3.4 职业性别的刻板印象
- GPT 输出的职务描述存在性别偏见（如“幼教=女性”）
- https://textio.com/blog/chatgpt-writes-performance-feedback

### 🗳️ 3.5 政治立场偏移
- 多数的语言模型都是左派偏向自由主义
- 《[The Political Preferences of LLMs](https://arxiv.org/abs/2402.01789)》

### 🌉 3.6 偏见缓解研究
- 关于大型语言模型缓解偏见问题的研究综述
- 《[Bias and Fairness in Large Language Models: A Survey](https://arxiv.org/abs/2309.00770)》

---

## 🕵️‍♂️ 四、判断句子是否由 AI 生成

- 将 AI 生成的内容和人类生成的文本内容比较，寻找差异。
  - 《[DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature](https://arxiv.org/abs/2301.11305)》
  - 《[DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text](https://arxiv.org/abs/2305.17359)》
  - 《[Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts](https://arxiv.org/abs/2306.04723)》


- 训练文本分类器：AI / Human。
  - 《[Testing of Detection Tools for AI-Generated Text](https://arxiv.org/abs/2306.15666)》
  - 《[Can AI-Generated Text be Reliably Detected?](https://arxiv.org/abs/2303.11156)》

- 有研究统计多个会议审稿意见中包含 AI 生成的痕迹，2022 GPT-3 出现后这种趋势非常明显。
  - 《[Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews](https://arxiv.org/abs/2403.07183)》

---



## 🧬 五、输出加水印的技术

### 🧷 5.1 水印机制
- 调整特定 token 的出现概率，在输出中加入人眼难以察觉的特征水印，用以识别 AI 内容
- 《[A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226)》
- 《[On the Reliability of Watermarks for Large Language Models](https://arxiv.org/abs/2306.04634)》

### 💣 5.2 水印破坏与对抗研究
- 研究者也在尝试移除或绕过水印
- 《[Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense](https://arxiv.org/abs/2303.13408)》
- 《[Can AI-Generated Text be Reliably Detected?](https://arxiv.org/abs/2303.11156)》

---

## 🧾 课堂小结

本节课系统介绍了大型语言模型在现实应用中面临的关键安全议题，包括“幻觉”与事实错漏、内嵌偏见与社会歧视、AI 生成内容的可识别性、以及模型在招聘、学术等实际场景下可能带来的伦理影响。同时也介绍了前沿的检测与缓解方法（如事实查核工具与水印机制），带我们快速浏览了安全性议题领域正在解决的问题。

---


