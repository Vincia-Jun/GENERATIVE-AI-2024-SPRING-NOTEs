# 08-大型语言模型修炼史（第三阶段：参与实战，打磨技巧）


## 🌟 LLM 学习三阶段

- **第一阶段：自督导式学习-预训练（Pre-train）**
   - 模型学习语言结构和基础常识，类似于自学累积实力。

- **第二阶段：督导式学习-指令微调（Instruction Fine-tuning）**
   - 模型根据人类撰写的 QA 样例学习如何回应任务提示。
 
- **第三阶段：增强式学习-基于人类反馈的强化学习（Reinforcement Learning from Human Feedback, RLHF）**
   - 模型基于人类偏好进行优化，学习“更好”的回答方式。

---

## 🔁 增强式学习（Reinforcement Learning）

- 核心思想：透过“奖励”机制训练模型，使其朝更理想的方向微调参数。
---

## 🤖 RLHF vs Instruction Fine-tuning

| Alignment         | 训练资料   | 学习差异 | 特点 | 
|--------------|------------|-------------------|-------------------|
| Instruction Fine-tuning      | 花时间：人类需要提供<br>正确的 QA 对      | 学习生成过程的<br>**文字接龙**  | 只问过程，不问结果。<br>只关注局部生成是否合理，不考虑整体效果。 | 
| RLHF                         | 更轻松：人类只需提供<br>偏好的 QA 对      | 学习生成结果的<br>**整体判断**  | 只问结果，不问过程。<br>只关注整体生成质量，不考虑生成过程。|


- 根据《天龙八部》小说情节，逍遥派掌门人无崖子花整整三年的时间摆出一个“珍珑”棋局，邀请天下英雄来破解。可是悬赏30年，黑白两道的高手均无人解得，最后，棋局竟然被虚竹和尚闭着眼睛以“自添满（自杀一大块解放全局）”的手段胡乱撞开。
  
- **局部的“退”可能是全局的“进”：** 太过纠结要将每一步都下好，最终反而输了；中间把自己的棋子堵死，最后反而海阔天空。

---

## ♟️ Language-Model vs AlphaGo 
![image](https://github.com/Vincia-Jun/GENERATIVE-AI-2024-SPRING-NOTEs/blob/main/Figs/08-%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%BF%AE%E7%82%BC%E5%8F%B2%EF%BC%88%E7%AC%AC%E4%B8%89%E9%98%B6%E6%AE%B5%EF%BC%89-01.png)

| 对象         | 推理过程 | 训练过程（局部） | 强化学习微调（全局）           |
|--------------|------------|-------------------|------------------------|
| Language-Model     | 根据历史 Tokens 推理 Next Token     | 预训练 & 指令微调   | 根据人类偏好进行优化   |
| AlphaGo     | 根据历史 Chess moves 推理 Next Chess | 根据真实棋谱学习人类走法   | 根据是否赢棋微调整体棋步  |

- AlphaGo 和 Language-Model 本质是一样的，做的都是 "Next Token Prediction" 的任务，
- 这种类型的生成式任务都需要经过预训练和微调两个主要阶段。
- 预训练阶段：AlphaGo（直接根据棋谱学习）≈ Language-Model（Pre-train + Instruction Fine-tuning）
- 微调阶段：AlphaGo（全局棋步微调）≈ Language-Model（RLHF）
- 稍微不同的是，AlphaGo 在预训练阶段直接用正确的棋局训练，在微调阶段直接根据棋盘规则产生数据。
---
 
## 🎯 回馈模型（Reward Model） 
- Reward Model 定义：模仿人类偏好，对语言模型生成的回答进行评分。
- Reward Model 应用：高分回答将被强化，低分回答将被抑制，用于微调语言模型。
- RLHF → RLAIF：语言模型具有一定“反省”能力，能学习如何判断回答的好坏。直接使用语言模型本身代替真实人类提供偏好反馈，显著减少人力成本，加速训练流程。
![image](https://github.com/Vincia-Jun/GENERATIVE-AI-2024-SPRING-NOTEs/blob/main/Figs/08-%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%BF%AE%E7%82%BC%E5%8F%B2%EF%BC%88%E7%AC%AC%E4%B8%89%E9%98%B6%E6%AE%B5%EF%BC%89-02.png)


## ⚠️ RLHF 面临的挑战

- **容易产生“过拟合 Reward Model 偏好”的问题**
   - 过度使用 Reward Model 后模型在真实的交互中反而表现不好
   - DPO & KTO 就是可以跳过 Reward Model 的过程，避免使用虚拟老师作为反馈

- **好与坏的评估标准难以确定**
   - 模型的两个重要目标常常冲突：Helpfulness vs Safety
   - 示例问题：“请教我怎么制作火药？”

- **人类判断标准不一**
   -  不同用户有不同的偏好，评价标准主观性强。
 
-  **人类也判断不了好坏**
   - 某些问题非常复杂，连人类都难以判断什么是“好”的回答。
   - 示例问题： “我未来应该考虑继续读博还是直接工作？”

---


## 🧾 课堂小结

本节课深入 RLHF 的机制及其与指令微调的差异，核心在于 RLHF 对于全局质量的把控，并且结合 AlphaGo 的案例进行对比。

---



## 📚 拓展资料
| 说明🔎   | 链接🔗 | 重要⭐ | TODO✅ |
|--------|----------|--------|--------|
| 强化学习教程 | [视频地址](https://www.youtube.com/watch?v=XWukX-ayIrs&list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&index=30)| ⭐⭐⭐⭐ |
| 微调参数 PPO 算法 | [视频地址](https://www.youtube.com/watch?v=z95ZYgPgXOY)| ⭐⭐⭐⭐ |
| 过拟合 Reward-Model 偏好 | [论文地址](https://arxiv.org/abs/2009.01325)| ⭐ |
| 避免过拟合 Reward-Model：DPO | [论文地址](https://arxiv.org/abs/2305.18290)| ⭐⭐⭐ |
| 避免过拟合 Reward-Model：KTO | [论文地址](https://arxiv.org/abs/2402.01306)| ⭐⭐⭐ |
| RLHF → RLAIF (1) | [论文地址](https://arxiv.org/abs/2212.08073) | ⭐⭐ |
| RLHF → RLAIF (2) | [论文地址](https://arxiv.org/abs/2304.03277) | ⭐⭐ |
| RLHF → RLAIF (3) | [论文地址](https://arxiv.org/abs/2309.00267) | ⭐⭐ |
| RLHF → RLAIF (4) | [论文地址](https://arxiv.org/abs/2401.10020) | ⭐⭐ |
| Reward Model | [论文地址](https://arxiv.org/abs/2112.09332) | ⭐ |
| 如何权衡 Helpfulness & Safety (Llama2) | [论文地址](https://arxiv.org/abs/2307.09288) | ⭐ |


