# 08-大型语言模型修炼史（第三阶段：参与实战，打磨技巧）


## 🌟 LLM 学习三阶段

- **第一阶段：自督导式学习-预训练（Pre-train）**
   - 模型学习语言结构和基础常识，类似于自学累积实力。

- **第二阶段：督导式学习-指令微调（Instruction Fine-tuning）**
   - 模型根据人类撰写的 QA 样例学习如何回应任务提示。
 
- **第三阶段：增强式学习-基于人类反馈的强化学习（Reinforcement Learning from Human Feedback, RLHF）**
   - 模型基于人类偏好进行优化，学习“更好”的回答方式。

---

## 🔁 增强式学习（Reinforcement Learning）

- 核心思想：透过“奖励”机制训练模型，使其朝更理想的方向微调参数。
- 例子：
  - 问题：“台湾最高的山是哪座？”
  - 回答 A：“玉山” → 提高概率
  - 回答 B：“谁来告诉我呀” → 降低概率

---

## 🤖 RLHF vs Instruction Fine-tuning

| Alignment         | 训练资料   | 学习差异 | 特点 | 
|--------------|------------|-------------------|-------------------|
| Instruction Fine-tuning      | 花时间：人类需要提供<br>正确的 QA 对      | 学习生成过程的<br>**文字接龙**  | 只问过程，不问结果。<br>只关注局部生成是否合理，不考虑整体效果。 | 
| RLHF                         | 更轻松：人类只需提供<br>偏好的 QA 对      | 学习生成结果的<br>**整体判断**  | 只问结果，不问过程。<br>只关注整体生成质量，不考虑生成过程。|


- 根据《天龙八部》小说情节，逍遥派掌门人无崖子花整整三年的时间摆出一个“珍珑”棋局，邀请天下英雄来破解。可是悬赏30年，黑白两道的高手均无人解得，最后，棋局竟然被虚竹和尚闭着眼睛以“自添满（自杀一大块解放全局）”的手段胡乱撞开。
- **局部的“退”可能是全局的“进”：** 太过纠结要将每一步都下好，最终反而输了；中间把自己的棋子堵死，最后反而海阔天空。

---

## ♟️ 语言模型 vs AlphaGo 

| 对象         | 推理过程 | 训练过程（局部） | 强化学习（全局）           |
|--------------|------------|-------------------|------------------------|
| 语言模型     | 根据历史 Tokens <br>推理 Next Token     | 预训练 & 指令微调         | 根据人类偏好进行优化   |
| AlphaGo     | 根据历史 Chess moves <br>推理 Next Chess | 根据真实棋谱学习人类走法   | 根据是否赢棋微调整体棋步        |


- AlphaGo 根据棋谱模仿人类走法
- LLM 在前两阶段也模仿人类说法（“说人说的话”）
- 在 RL 阶段尝试输出更优文本（“说人喜欢听的话”）

- 整体上是生成式学习，局部是分类问题。
- 理解 Aplgo 的两个学习阶段过程：也是文字接龙 & 整体判断。
- 大模型是三个阶段，非常类似的，只是在 RL 的阶段的奖励是稍微有去别的。

---
 
## 🎯 回馈模型（Reward Model） 

- 模仿人类偏好对模型输出打分。
- 输入：多个生成答案
- 输出：每个答案的“得分”
- 模型根据得分微调参数 → 提高好答案概率，压低差答案概率
- 也可以作为一种 zero-shot 质量评分的输出判断。

> 示例：  
> 问题：“世界上最高的山是哪座？”  
> 回答 A：“我不知道这个问题的答案。” → 低分  
> 回答 B：“喜马拉雅山。” → 高分

- 效果如何呢，可以通过 Instruct GPT 判断好坏
---



- 但可能出现“过拟合虚拟人类偏好”的问题：
- 虚拟学习的一些后遗症
- DPO & KTO 就是可以跳过 REward-Model 的过程，避免使用虚拟老师
---

## 🚀 RLHF 的演化路径

- **InstructGPT**：初代尝试
- **RLHF → RLAIF（Reinforcement Learning from AI Feedback）**：
  - 直接用 **语言模型** 代替真实人类给反馈，节省人力，语言模型有反省能力 
    

---



## ⚠️ RLHF 面临的挑战

1. **好与坏的标准？**
   - Helpfulness vs Safety 难以平衡
   - 示例：“请教我怎么制作火药。” → 安全模型需要能拒绝
2. **人类判断标准不一**
3. - 人类自己都判断不了好坏怎么办？语言模型要怎么进步呢？ 
   - 不同人对“好回答”的定义差异大
   - Prompt 的表述方式对模型输出影响显著




## 📚 推荐阅读资料

- [InstructGPT: 2203.02155](https://arxiv.org/abs/2203.02155)
- [DPO: 2305.18290](https://arxiv.org/abs/2305.18290)
- [RLHF→RLAIF: 2212.08073](https://arxiv.org/abs/2212.08073)
- [Llama2: 2307.09288](https://arxiv.org/abs/2307.09288)
- [Helpfulness vs Safety: 2204.05862](https://arxiv.org/pdf/2204.05862.pdf)





## Ⅶ，拓展资料
| 说明🔎   | 链接🔗 | 重要⭐ | TODO✅ |
|--------|----------|--------|--------|
| 强化学习教程 | [视频地址](https://www.youtube.com/watch?v=XWukX-ayIrs&list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&index=30)| ⭐⭐⭐⭐ |
| 微调参数 PPO 算法 | [视频地址](https://www.youtube.com/watch?v=z95ZYgPgXOY)|  |
| 过拟合 Reward-Model 偏好 | [论文地址](https://arxiv.org/abs/2009.01325)|  |
| 避免过拟合 Reward-Model：DPO | [论文地址](https://arxiv.org/abs/2305.18290)|  |
| 避免过拟合 Reward-Model：KTO | [论文地址](https://arxiv.org/abs/2402.01306)|  |

| RLHF → RLAIF (1) | [论文地址](https://arxiv.org/abs/2212.08073) |  |
| RLHF → RLAIF (2) | [论文地址](https://arxiv.org/abs/2304.03277) |  |
| RLHF → RLAIF (3) | [论文地址](https://arxiv.org/abs/2309.00267) |  |
| RLHF → RLAIF (4) | [论文地址](https://arxiv.org/abs/2401.10020) |  |

| LLMAa2 | [论文地址]() |  |




|  | [网页地址]() |  |
|  | [网页地址]() |  |
|  | [网页地址]() |  |

|  | [论文地址]() |  |
|  | [论文地址]() |  |
|  | [论文地址]() |  |
|  | [论文地址]() |  |
