# 06-大型语言模型修炼史（第一阶段：自我学习，累积实力）

## Ⅰ，背景知识
- **文字接龙：** Next Token Prediction 是 ChatGPT 的核心原理，即通过预测未完成句子的下一个字来生成文本。例如，输入“台湾大”，它会预测接下来可能是“学”、“哥”等字，并将置信度转为概率分布选择输出字。

- **参数优化：** 训练模型的本质就是参数优化，通过调整参数使模型符合训练数据。
  - 随机的初始参数：train from scratch → 更难找到最佳参数。
  - 调优的初始参数：依赖于先验知识 → 更快找到最佳参数。

- **过拟合困境：** 模型仅匹配训练数据，但测试效果差。
  - 数据层面：增加数据多样性。
  - 训练层面：调整超参数重新训练。
  - 模型层面：引入先验知识优化初始化参数。
![image](./Figs/06-%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%BF%AE%E7%82%BC%E5%8F%B2%EF%BC%88%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%EF%BC%89-00.png)

## Ⅱ，数据准备
- **训练资料：** 互联网上的任何文字资料都可以用来进行 “文字接龙” 预训练。
  
- **数据清洗：** 过滤有害内容、去除 HTML 标签、筛选高质量数据、去除重复资料。其中，GPT-3 等模型训练过程，会额外训练出一个 “资料品质” 分类器，用于选出 wiki 类型的高品质语料。
  
- **数据安全：** 纽约时报曾起诉 OpenAI 的训练资料含有未授权内容，随便乱爬可能会遭遇版权问题，不过其实大部分公司都是随便爬来就用。
![image](./Figs/06-%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%BF%AE%E7%82%BC%E5%8F%B2%EF%BC%88%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%EF%BC%89-01.png)

## Ⅲ，GPT 发展历程
| 模型   | 发布时间 | 参数量（天资）   | 数据量（努力）   | 主要特点                     |
|--------|----------|----------|----------|--------------------------|
| GPT-1  | 2018     | 117M     | 1G      |  仅限于简单文本生成        |
| GPT-2  | 2019     | 1.5B     | 40GB    | 可以进行基本的问答与摘要           |
| GPT-3  | 2020     | 175B     | 580GB   | 能够生成较复杂的文章，代码等   |
| GPT-4  | 2023     | 未公开   | 未公开   | 多模态对齐人类需求，更强的推理能力  |

- GPT-3 虽然能力强，但由于只从互联网学习，容易生成非预期的内容。
- GPT-3 效果差强人意，2020年许多人认为 OpenAI 走错了方向。

## Ⅳ，课堂小结
大型语言模型通过海量数据学习“文字接龙”任务，逐步掌握语言规则与知识，但其核心瓶颈在于缺乏对知识的深层理解与灵活应用能力，类似于拥有深厚内功却未习得具体招式的武者，难以将所学有效转化为实际问题的解决方案。

## Ⅴ，拓展资料
| 说明🔎   | 链接🔗 | 重要⭐ | TODO✅ |
|--------|----------|--------|--------|
| 神经网络如何分辨**宝可梦**和**数码宝贝**的？ | [视频地址](https://www.youtube.com/watch?si=DR8fnpmbvi7bmfsn&t=1535&v=WQY85vaQfTI&feature=youtu.be) | ⭐⭐⭐⭐ |
| 学习文字接龙需要多少文字？ | [论文地址](https://arxiv.org/abs/2011.04946) | ⭐ |
| Deepmind 训练 Gopher 过程  | [论文地址](https://arxiv.org/abs/2112.11446) | ⭐⭐ |
| 预训练去除重复语料 | [论文地址](https://arxiv.org/abs/2107.06499) | ⭐ |
